<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-04-18">18 Apr 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-04-18">18 Apr 2023</date>
						</imprint>
					</monogr>
					<idno type="MD5">232C2C76E7CADA2373394C1C585C22C9</idno>
					<idno type="arXiv">arXiv:2304.05133v2[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2025-08-03T20:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Machine Learning (ML) denotes the field of study in which algorithms infer from given data how to perform a specific task, without being explicitly programmed for the task <ref type="bibr">(Arthur Samuel, 1959)</ref>. Here, we consider a popular subset of ML algorithms: Neural Networks. The inspiration for a Neural Network (NN) originates from the human brain, where biological neurons (nerve cells) respond to the activation of other neurons they are connected to. At a very simple level, neurons in the brain take electrical inputs that are then channeled to outputs. The sensitivity of this relation also depends on the strength of the connection, i.e. a neuron may be more responsive to one neuron, then to another. Brain Neuron Structure: electrical inputs are received through dendrites and transmitted via the axon to other cells. There are approximately 86 billion neurons in the human brain. Image modified from: https://www.smartsheet.com/ neural-network-applications.</p><p>For a single neuron/node with input u ∈ R n , a mathematical model, named the perceptron <ref type="bibr" target="#b26">[27]</ref>, can be described as</p><formula xml:id="formula_0">y = σ n i=1 W i u i + b = σ(W u + b),<label>(1)</label></formula><p>where y is the activation of the neuron/node, W i are the weights and b is the bias.</p><formula xml:id="formula_1">y [0] 1 y [0] 2 y [0] 3 y [1] 1</formula><p>input layer output layer Figure <ref type="figure">2</ref>. Schematic representation of the perceptron with a three dimensional input. For generality we denote the input by y [0] = u and the output by y [1] = y. The weights W i are applied on the arrows and the bias is added in the node y <ref type="bibr" target="#b0">[1]</ref> 1 .</p><p>The function σ : R → R is called activation function. Originally, in <ref type="bibr" target="#b26">[27]</ref>, it was proposed to choose the Heaviside function as activation function to model whether a neuron fires or not, i.e.</p><formula xml:id="formula_2">σ(y) = 1 if y ≥ 0, 0 if y &lt; 0.</formula><p>However, over time several other activation functions have been suggested and are being used. Typically, they are monotone increasing to remain in the spirit of the original idea, but continuous.</p><p>Popular activation functions are, cf. <ref type="bibr">[26, p.90]</ref> σ(y) = 1 1 + exp(-y) sigmoid (logistic), Remark 1.1 The nonlinearity of activation functions is an integral part of the Neural Networks success. Since concatenations of linear functions result again in a linear function, see e.g. <ref type="bibr">[26, p.90</ref>], the complexity that can be achieved by using linear activation functions is limited.</p><p>While the sigmoid function approximates the Heaviside function continuously, and is differentiable, it contains an exponential operation, which is computationally expensive. Similar problems arise with the hyperbolic tangent function. However, the fact that tanh is closer to the identity function often helps speed up convergence, since it resembles a linear model, as long as the values are close to zero. Another challenge that needs to be overcome is vanishing derivatives, which is visibly present for Heaviside, sigmoid and hyperbolic tangent. In contrast, ReLU is not bounded on positive values, while also being comparatively cheap to compute, because linear computations tend to be very well optimized in modern computing. Altogether, these advantages have resulted in ReLU (and variants thereof) becoming the most widely used activation function currently. As a remedy for the vanishing gradient on negative values, leaky ReLU was introduced. When taking derivatives of ReLU one needs to account for the non-differentiability at 0, but in numerical practice this is easily overcome.</p><p>With the help of Neural Networks we want to solve a task, cf. <ref type="bibr" target="#b14">[15,</ref><ref type="bibr">Section 5.1]</ref>. Let the performance of the algorithm for the given task be measured by the loss function L, which needs to be adequately modeled. By F we denote the Neural Network. The variables that will be learned are the weights W and biases b of the Neural Network. Hence, we can formulate the following optimization problem, cf. One possible choice for F has already been given in <ref type="bibr" target="#b0">(1)</ref>, the perceptron. In the subsequent sections we introduce and analyze various other Neural Network architectures. They all have in common that they contain weights and biases, so that the above problem formulation remains sensible.</p><p>Before we move on to different network architectures, we discuss the modeling of the loss function.</p><p>Learning tasks can be divided into two subgroups: Supervised and Unsupervised learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Supervised Learning</head><p>In supervised learning we have given data u with known supervision S(u) (also called labels), so that the task is to match the output y of the Neural Network to the supervision. These problems are further categorized depending on the known supervision, e.g. for S(u) ∈ N it is called a classification and for S(u) ∈ R a regression. Furthermore, the supervision S(u) can also take more complex forms like a black and white picture of 256 × 256 pixels represented by [0, 1] 256 , a higher dimensional quantity, a sentence, etc. These cases are called structured output learning.</p><p>Let us consider one very simple example, cf. <ref type="bibr" target="#b14">[15,</ref><ref type="bibr">Section 5.1.4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example 1.2 Linear Regression</head><p>We have a given set of inputs u (i) ∈ R d with known supervisions S(u (i) ) ∈ R for i = 1, . . . , N .</p><p>In this example we only consider weights W ∈ R d and no bias. Additionally, let σ = id. The perceptron network simplifies to</p><formula xml:id="formula_3">y (i) = W u (i) ,</formula><p>and the learning task is to find W , such that y (i) ≈ S(u (i) ). This can be modeled by the mean squared error (MSE) function</p><formula xml:id="formula_4">L ({y (i) } i , {u (i) } i , W ) := 1 2N N i=1 y (i) -S(u (i) ) 2 .</formula><p>By convention we will use • = • 2 throughout the lecture. The chosen loss function is quadratic, convex and non-negative. We define</p><formula xml:id="formula_5">U :=   </formula><p>(u (1) ) . . .</p><formula xml:id="formula_6">(u (N ) )    ∈ R N ×d , S :=   </formula><p>S(u (1) ) . . .</p><formula xml:id="formula_7">S(u (N ) )    ∈ R N ,</formula><p>so that we can write L (W ) = 1 2 U W -S 2 2 . Minimizing this function will deliver the same optimal weight W as minimizing the MSE function defined above. We can now derive the gradient</p><formula xml:id="formula_8">∇ W L (W ) = U U W -U S</formula><p>and immediately find the stationary point W = (U U ) -1 U S.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">Unsupervised Learning</head><p>In unsupervised learning, only the input data u is given and we have no knowledge of supervisions or labels. The algorithm is supposed to learn e.g. a structure or relation in the data. Some examples are k-clustering and principal component analysis (PCA). Modeling the loss function specifies the task and has a direct influence on the learning process. For illustration of this concept, we introduce the k-means algorithm, see eg. <ref type="bibr" target="#b25">[26,</ref><ref type="bibr">Chapter 10]</ref>, which is used for clustering.</p><p>Example 1. <ref type="bibr" target="#b2">3</ref> We have a set of given data points</p><formula xml:id="formula_9">u (i) N i=1 ∈ R d ,</formula><p>and a desired number of clusters k ∈ N with k ≤ N and typically k N . Every data point is supposed to be assigned to a cluster. Iteratively every data point is assigned to the cluster with the nearest centroid, and we redefine cluster centroids as the mean of the vectors in the cluster. The procedure is specified in Algorithm 1 and illustrated for an example in Figure <ref type="figure" target="#fig_3">4</ref>, which can be found e.g. in <ref type="bibr" target="#b25">[26,</ref><ref type="bibr">Chapter 10]</ref>. The loss function (also called distortion function in this setup) can be defined as</p><formula xml:id="formula_10">L (c, µ) := N i=1 u (i) -µ c (i) 2 ,</formula><p>which is also a model of the quantity that we try to minimize in Algorithm 1. We have a nonconvex set of points in R d , so the algorithm may converge to a local minimum. To prevent this, we run the algorithm many times, compare the resulting clusterings using the loss function, and choose the one with the minimal value attained in the loss function. We will see various other loss functions L throughout the remainder of this lecture, all of them specifically tailored to the task at hand.</p><p>In the case of Linear Regression, we have a closed form derivative, so we are able to find the solution by direct calculus, while for k-means clustering the optimization was done by a tailored iteration. For general problems we will need a suitable optimization algorithm. We move on to introduce a few options.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 k-means clustering</head><p>Require: Initial cluster centroids µ 1 , . . . , µ k while not converged do for i = 1 : N do</p><formula xml:id="formula_11">c (i) := arg min j u (i) -µ j 2 end for for j = 1 : k do µ j ← N i=1 1 {c (i) =j} u (i) N i=1 1 {c (i) =j}</formula><p>end for end while</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3.">Optimization Algorithms</head><p>Here, for simplicity we define θ, which collects all variables, i.e. weights W and bias b and write the loss function as</p><formula xml:id="formula_12">L (θ) = 1 N N i=1 L (i) (θ),</formula><p>which we want to minimize. Here, L (i) indicates the loss function evaluated for data point i, for example with a MSE loss</p><formula xml:id="formula_13">L (i) (θ) = 1 2 y (i) -S(u (i) ) 2 .</formula><p>First, let us recall the standard gradient descent algorithm, see e.g. <ref type="bibr" target="#b5">[6,</ref><ref type="bibr">Section 9.3]</ref>, which is also known as steepest descent or batch gradient descent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2 Gradient Descent</head><p>Require: Initial point θ 0 , step size τ &gt; 0, counter k = 0.</p><p>while Stopping criterion not fulfilled do</p><formula xml:id="formula_14">θ k+1 = θ k -τ • ∇L (θ k ), k ← k + 1.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>end while</head><p>Possible stopping criterion are e.g. setting a maximum number of iterations k, reaching a certain exactness L (θ) &lt; with a small number &gt; 0, or a decay in change θ k+1 -θ k &lt; . Determining a suitable step size is integral to the success of the gradient descent method, especially since this algorithm uses the same step size τ for all components of θ, which can be a large vector in applications. If may happen that in some components the computed descent direction is only providing descent in a small neighborhood, therefore requiring a small step size τ . It is also possible to employ a line search algorithm. However, this is not common in Machine Learning currently. Instead, typically a small step size is chosen, so that it will (hopefully) be not too large for any component of θ, and then it may be adaptively increased. Furthermore, let us remark that the step size is often called learning rate in a Machine Learning context.</p><p>Additionally, a grand challenge in Machine Learning tasks is that we have huge data sets, and the gradient descent algorithm has to iterate over all data points in every iteration, since L (θ) contains all data points, which causes a tremendous computational cost. This motivates the use of the stochastic gradient descent algorithm, cf. [26, <ref type="bibr">Algorithm 1]</ref>, which only takes one data point into account per iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 3 Stochastic Gradient Descent (SGD)</head><p>Require: Initial point θ 0 , step size τ &gt; 0, counter k = 0, maximum number of iterations K.</p><formula xml:id="formula_15">while k ≤ K do Sample j ∈ {1, . . . , N } uniformly. θ k+1 = θ k -τ • ∇L (j) (θ k ), k ← k + 1.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>end while</head><p>Since the stochastic gradient descent method only calculates the gradient for one data point, it produces an irregular convergence behavior. Indeed, it does not necessarily converge at all, but for a large number of iterations K it often produces a good approximation. In fact, actually converging in training the Neural Network is often not necessary/desired anyhow, since we want to have a solution that generalizes well to unseen data, rather than fit the given data points perfectly. Actually, the latter may lead to overfitting, cf. Section 1.4. Therefore, SGD is a computationally cheap, reasonable alternative to gradient descent. As a compromise, which generates a less irregular convergence behavior, there also exists mini batch gradient descent, cf. <ref type="bibr" target="#b25">[26,</ref><ref type="bibr">Algorithm 2]</ref>, where every iteration takes into account a subset (mini batch) of the data points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 4 Mini Batch Gradient Descent</head><p>Require: Initial point θ 0 , step size τ &gt; 0, counter k = 0, maximum number of iterations K, batch size b ∈ N.</p><formula xml:id="formula_16">while k ≤ K do Sample b examples j 1 , . . . , j b uniformly from {1, . . . , N } θ k+1 = θ k -τ • 1 b b i=1 ∇L (ji) (θ k ), k ← k + 1.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>end while</head><p>Finally, we introduce a sophisticated algorithm for stochastic optimization called Adam, <ref type="bibr" target="#b22">[23]</ref>, see Algorithm 5. It is also a gradient-based method, and as an extension of the previous methods it employs adaptive estimates of so-called moments. Good default settings in Adam for the tested machine learning problems are τ = 0.001, β 1 = 0.9, β 2 = 0.999 and = 10 -8 , cf. <ref type="bibr" target="#b22">[23]</ref>. Typically, the stochasticity of L (θ) will come from using mini batches of the data set, as in Mini Batch Gradient Descent, Algorithm 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 5</head><p>Adam. All operations on vectors are element-wise. (g k ) 2 indicates the elementwise square g k g k , and (β 1 ) k , (β 2 ) k denote the k-th power of β 1 and β 2 , respectively.</p><p>Require: Initial point θ 0 , step size τ &gt; 0, counter k = 0, exponential decay rates for the moment estimates β 1 , β 2 ∈ [0, 1), &gt; 0, stochastic approximation L (θ) of the loss function.</p><p>m 0 1 ← 0 (Initialize first moment vector) m 0 2 ← 0 (Initialize second moment vector) while θ k not converged do <ref type="bibr" target="#b3">4</ref> In any case we need to be cautious when interpreting results, since independent of the chosen algorithm, we are dealing with a non-convex loss function, so that we can only expect convergence to stationary points. In the following section we discuss how fitting the given data points and generalizing well to unseen data can be contradictory goals.</p><formula xml:id="formula_17">g k+1 = ∇ θ L (θ k ) m k+1 1 = β 1 • m k 1 + (1 -β 1 ) • g k+1 m k+1 2 = β 2 • m k 2 + (1 -β 2 ) • (g k+1 ) 2 m k+1 1 ← m k+1 1 (1-(β1) k ) m k+1 2 ← m k+1 2 (1-(β2) k ) θ k+1 = θ k -τ • m k+1 1 √ m k+1 2 + k ← k + 1 end while Remark 1.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.4.">Overfitting and Underfitting</head><p>As an example we discuss supervised learning with polynomials of degree r, cf. <ref type="bibr" target="#b11">[12,</ref><ref type="bibr">Section 1.3.3]</ref>.</p><formula xml:id="formula_18">Example 1.5 Define p(u, W ) := r j=0 W j u j = W u,</formula><p>with u = (u 0 , ..., u r ) ∈ R r+1 the potencies of data point u, and W := (W 0 , ..., W r ) ∈ R r+1 . The polynomial p is linear in W , but not in u. As in Linear Regression (Example 1.2), we do not consider bias b here. Our goal is to compute weights W , given data points u (i) with supervisions S(u (i) ), so that p makes good predictions on data it hasn't seen before. We again employ the MSE loss function</p><formula xml:id="formula_19">L (W ) = 1 2N N i=1 p(u (i) , W ) -S(u (i) ) 2</formula><p>As before, we write the loss in matrix-vector notation</p><formula xml:id="formula_20">L (W ) = 1 2N U W -S 2</formula><p>where S(u (1) ) . . .</p><formula xml:id="formula_21">U :=     u (1) 0 u (1) 1 . . . u<label>(</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S(u (m)</head><p>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>  </head><p>The minimizer W can be directly calculated, cf. Example 1.2. To measure the performance of the polynomial curve fitting we compute the error on data points that were not used to determine the best polynomial fit, because we aim for a model that will generalize well. To this end, finding a suitable degree for the polynomial that we are fitting over the data points is crucial. If the degree is too low, we will encounter underfitting, see Figure <ref type="figure" target="#fig_6">6</ref> top row. This means that the complexity of the polynomial is too low and the model does not even fit the data points. A remedy is to increase the degree of the polynomial, see Figure <ref type="figure" target="#fig_6">6</ref> bottom left. However, increasing the degree too much may lead to overfitting, see Figure <ref type="figure" target="#fig_6">6</ref> bottom right. The data points are fit perfectly, but the curve will not generalize well.</p><p>We can characterize overfitting and underfitting by using some statistics, cf. <ref type="bibr" target="#b25">[26,</ref><ref type="bibr">Section 8.1]</ref>.</p><p>A point estimator g : U N → Θ (where U denotes the data space, and Θ denotes the parameter space) is a function which makes an estimation of the underlying parameters of the model. For example, the estimate for θ = W from Example 1.2: θ = (U U ) -1 U S (which we will denote with a hat in this subsection to emphasize that it is an estimation) is an example of a point estimator. We assume that the data from U N is i.i.d, so that θ is a random variable. We can define the variance and the bias</p><formula xml:id="formula_22">Var( θ) := E( θ2 ) -E( θ) 2 , Bias( θ) := E( θ) -θ,</formula><p>with E denoting the expected value. A good estimator has both, low variance and low bias. We can characterize overfitting with low bias and high variance, and underfitting with high bias and low variance. The bias-variance trade-off is illustrated in Figure <ref type="figure" target="#fig_7">7</ref>. Hence, we can make a decision based on mean squared error of the estimates In general, it can be hard to guess a suitable degree for the polynomial beforehand. We could compute a fitting curve for different choices of r and then compare the error on previously unseen data points of the validation data set, cf. Section 1.5, to determine which one generalizes best. This will require solving the problem multiple times which is unfavorable, especially for large data sets. Also, the polynomial degree can only be set discretely. Another, continuous way is to introduce a penalization term in the loss function</p><formula xml:id="formula_23">MSE( θ) := E[( θ -θ) 2 ] = Var( θ) + Bias( θ) 2 .</formula><formula xml:id="formula_24">L λ (θ) := L (θ) + λ θ 2 .</formula><p>This technique is also called weight decay, cf. <ref type="bibr" target="#b14">[15,</ref><ref type="bibr">Section 5.2.2]</ref>. We can also use other norms, e.g. • 1 . Here, we can choose a large degree r and for λ big enough, we will still avoid overfitting, because many components of θ will be (close to) zero. Nonetheless, we need to be cautious with the choice of λ. If it is too big, we will face again the problem of underfitting.</p><p>We see that choosing values for the degree r and the penalization parameter λ poses challenges, and will discuss this further in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.5.">Hyperparameters and Data Set Splitting</head><p>We call all quantities that need to be chosen before solving the optimization problem hyperparameters, cf. <ref type="bibr" target="#b14">[15,</ref><ref type="bibr">Section 5.3]</ref>. Let us point out that hyperparameters are not learnt by the optimization algorithm itself, but nevertheless have an impact on the algorithms performance. Examples of hyperparameters include the polynomial degree r, the scalar λ, all parameters in the optimization algorithms (Section 1.3) like the step size τ , and also the architecture of the Neural Network, and many more.</p><p>The impact of having a good set of hyperparameters can be tremendous, however finding such a set is not trivial. First of all, we split our given data into three sets. training data, validation data and test data (a 4:1:1 ratio is common). We have seen training and test data before. The data points that we are using as input to solve the optimization problem are called training data, and the unseen data points, which we use to evaluate whether the model generalizes well, are called test data. Since we don't want to mix different causes of error, we also introduce the validation data set. This will be used to compare different choices of hyperparameter configurations, i.e. we train the model on the training data for different hyperparameters, compute the error on the validation data set, choose the hyperparameter setup with the lowest error and finally evaluate the model on the test set. The reasoning behind this is that if we would use the test data set to determine the hyperparameter values, the test error may be not meaningful, because the hyperparameters have been optimized for this specific test set. Since we are using the validation set, we will have the test set with previously unseen data available to determine the generalization error without giving our network an advantage.</p><p>Still, imagine you need to choose 5 hyperparameters and have 4 possible values that you want to try for each hyperparameter. This amounts to 4 5 = 1024 combinations you have to run on the training data and evaluate on the validation set. In real applications the number of hyperparameters and possible values can be much larger, so that it is nearly infeasible to try every combination, but rather common to change one hyperparameter at a time. Luckily, some hyperparameters also have known good default values, like the hyperparameters for Adam Optimizer, Algorithm 5. Apart from that it is a tedious, manual work to try out, monitor and choose suitable hyperparameters. Finally, we discuss the limitations of shallow Neural Networks, i.e. networks with only one layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.6.">Modeling logical functions</head><p>Let us consider a shallow Neural Network with input layer y [0] ∈ N 2 and output layer y [1] ∈ N. We model true by the value 1 and false by the value 0, which results in the following truth table for the logical "OR" function.</p><p>input y</p><formula xml:id="formula_25">[0] 1 input y [0] 2 y [0] 1 OR y [0] 2 (output y [1] 1 ) 0 0 0 1 0 1 0 1 1 1 1 1</formula><p>Table <ref type="table">1</ref>. Truth table for the logical "OR" function.</p><p>With Heaviside activation function, we have</p><formula xml:id="formula_26">y [1] 1 = 1, if W 1 y [0] 1 + W 2 y [0] 2 + b ≥ 0, 0, else .</formula><p>The goal is now to choose W 1 , W 2 , b so that we match the output from the truth table for given input. Obviously, W 1 = W 2 = 1 and b = -1 is a possible choice that fulfills the task. Similarly, one can find values for W 1 , W 2 and b to model the logical "AND" function.</p><p>Next, let us consider the logical "XOR" function with the following truth table.</p><p>input</p><formula xml:id="formula_27">y [0] 1 input y [0] 2 y [0] 1 XOR y [0] 2 (output y [1] 1 ) 0 0 0 1 0 1 0 1 1 1 1 0 Table 2.</formula><p>Truth table for the logical "XOR" function.</p><p>In fact, the logical "XOR" function can not be represented by the given shallow Neural Network, since the data is not linearly separable, see e.g. <ref type="bibr" target="#b14">[15,</ref><ref type="bibr">Section 6.1]</ref>. This motivates the introduction of additional layers in between the input and output layer, i.e. we choose a more complex function F in the learning problem (P ).</p><p>Figure <ref type="figure">9</ref>. This illustration shows that the logical "OR" function is linearly separable, while the logical "XOR" function is not. Image modified from: https://dev.to/jbahire/demystifyingthe-xor-problem-1blk.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Feedforward Neural Network</head><p>Introducing hidden layers, i.e. layers between the input and output layer, leads to Feedforward Neural Networks (FNNs), also called multilayer perceptrons (MLPs), cf. <ref type="bibr" target="#b14">[15,</ref><ref type="bibr">Section 6]</ref>. Essentially, they are multiple perceptrons organized in layers, = 0, . . . , L, where every perceptron takes the output from the previous perceptron as input. The number of layers L is called the depth of the network, while the number of neurons per layer n is the width of the network. The input layer is denoted with y [0] = u ∈ R n0 and not counted in the depth of the network. A FNN is called deep if it has at least two hidden layers. We now indicate the weights from layer to + 1 by W [ ] ∈ R n +1 ×n and the bias vector by b [ ] ∈ R n +1 for = 0, . . . , L -1.</p><p>To simplify notation, we extend the activation function σ : R → R to vector valued inputs, by applying it component-wise, so that σ : R n → R n , with (y 1 , . . . , y n ) → (σ(y 1 ), . . . , σ(y n )) .</p><p>The FNN layers can be represented in the same way as perceptrons</p><formula xml:id="formula_28">y [ ] = f (y [ -1] ) = σ [ ] (W [ -1] y [ -1] + b [ -1] ) for = 1, . . . , L,<label>(2)</label></formula><p>where the activation function σ [ ] may differ from layer to layer. We call y [ ] the feature vector of layer . Compactly, we can write a FNN as a composition of its layer functions, cf. [1, 2]</p><formula xml:id="formula_29">y [L] = F(u) = f L • f L-2 • . . . • f 1 (u).</formula><p>This formulation reinforces the choice of nonlinear activation function σ, cf. Remark 1.1. Otherwise, the output y [L] is linearly dependent on the input u and hidden layers can be eliminated. Hence, with linear activation function, solving rather simple tasks like modeling the logical "XOR" function will not be possible. However, sometimes it can be favorable to have one linear layer.</p><p>Remark 2.1 In practice, it is not uncommon that the last layer of a FNN is indeed linear, i.e. 1] . As long as the previous layers are nonlinear this does not hinder the expressiveness of the FNN, and is typically used to attain a desired output dimension. In essence, W [L-1] can be seen as a reformatting, in this case.</p><formula xml:id="formula_30">y [L] = W [L-1] y [L-</formula><p>However, in the remainder of this section we will consider the FNN architecture as introduced in (2). Let us now try again to represent the "XOR" logical function, this time by a FNN with one hidden layer.</p><formula xml:id="formula_31">y [0] 1 y [0] 2 y [1] 1 y [1] 2 y [2] 1</formula><p>input layer hidden layer output layer Figure <ref type="figure" target="#fig_0">10</ref>. A feedforward network with a two dimensional input and one hidden layer with 2 nodes, i.e.</p><formula xml:id="formula_32">n 0 = n 1 = 2, n 2 = 1 and L = 2.</formula><p>The variable choices</p><formula xml:id="formula_33">W [0] = 1 1 -1 -1 , b [0] = -1 1 , W [1] = 1 1 , b [1] = -2,</formula><p>solve the task and lead to the following truth table.</p><p>input</p><formula xml:id="formula_34">y [0] 1 input y [0] 2 y [1] 1 y [1] 2 y [0] 1 XOR y [0] 2 (output y [2] 1 ) 0 0 0 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0 0 Table 3.</formula><p>Truth table for the logical "XOR" function modeled by the FNN from Figure <ref type="figure" target="#fig_0">10</ref> and given variable choices as above.</p><p>Next, we formulate an optimization problem similar to (P ) for multiple layers with the above introduced notation, cf. <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref> min</p><formula xml:id="formula_35">{W [ ] } ,{b [ ] } L {y [L](i) } i , {u (i) } i , {W [ ] } , {b [ ] } (P ) s.t. y [L](i) = F u (i) , {W [ ] } , {b [ ] } .</formula><p>If we collect again all variables in one vector θ this will have the following length: The question that immediately arises is: How to choose the network architecture, i.e. depth and width of the FNN? The following discussion is based on [15, Section 6.4].</p><formula xml:id="formula_36">n 0 • n 1 + . . . + n L-1 • n L weights + n 1 + . . . + n L . biases y [0] 1 y [0] 2 y [0] 3 y [1] 1 y [1] 2 y [1] 3 y [1] 4 y [1] 5 y [2] 1 y [2] 2 y [2] 3 y [2] 4 y [2] 5 y [3] 1 y [3] 2 y [3] 3 y [3] 4 y [<label>3</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Depth and Width</head><p>The universal approximation theorem, see e.g. <ref type="bibr" target="#b9">[10]</ref>, states that any vector-valued, multivariate, measurable (in particular, continuous) function f : R n0 → R n L can be approximated with arbitrary small error by a Neural Network with one hidden layer. Hence, a first approach may be to choose a network with depth L = 2 and increase the width until the desired accuracy is reached.</p><p>However, this poses certain problems. First of all the universal approximation theorem does not imply that a training algorithm will actually reach the desired approximation, but rather that some set of parameters exists, that satisfies the requirement. The training algorithm might for example only find a local minimum or choose the wrong function as a result of overfitting. Another problem may be the sheer size of the layer required to achieve the wanted accuracy. In the worst case the network will need an exponential number of hidden units, with one hidden unit for each possible combination of inputs. Thus in practice, one is discouraged to use only one hidden layer, but instead to use deep networks.</p><p>Various families of functions are efficiently approximated by deep networks with a smaller width.</p><p>If one desires to approximate the same function to the same degree of accuracy, the number of hidden units typically grows exponentially. This stems from the fact, that each new layer allows the network to make exponentially more connections, thus allowing a wider output of target functions. Another reason why one might choose deeper networks is due to the intuition, that our desired function may well be a composition of multiple functions. Each new layer adds a nonlinear layer function to our network, thus making it easier for the FNN to approximate composite functions. Also, heuristically we observe that deep networks typically outperform shallow networks.</p><p>Nonetheless, one main problem with deep FNNs is, that the gradient used for training is the product of the partial derivatives of each layer, as we will see in Section 2.5. If these derivatives have small values, then the gradient for earlier layers can become very small. Thus training has a smaller if not even a negligible effect on the first layers when the network is too deep. This is especially a problem for sigmoid activation function, since its derivative is bounded by 1  4 . As discussed in Section 1.5, choosing hyperparameters like the depth and width is a non-trivial undertaking and currently, the method of choice is experimenting to find a suitable configuration for the given task.</p><p>Additionally, in the optimization algorithms, cf. Section 1.3, we need a starting point θ 0 for the variables. Hence, we discuss how to initialize the weights and biases in the FNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Initialization</head><p>Recall that due to the non-convex loss function, we can only expect convergence to stationary points, cf. Remark 1.4. Consequently, the choice of the initial point θ 0 can have great impact on the algorithm, since two different initial points can lead to two different results. An unsuitable initial point may even prevent convergence altogether. Similar to choosing hyperparameters, for the choice of initial points there exist several well-tested strategies, cf. [12, Section 4.2.2], but it is still an active field of research.</p><p>The naive approach would be to initialize θ = 0 or with some other constant value. Unfortunately, this strategy has major disadvantages. With this initialization, all weights per layer in the Neural Network have the same influence on the loss function and will therefore have the same gradient. This leads to all those neurons evolving symmetrically throughout training, so that different neurons will not learn different things, which significantly reduces the expressiveness of the FNN. Let us remark that it is fine to initialize the biases b [ ] with zero, as long as the weights W [ ] are not initialized constant. Hence, it is sufficient to discuss initialization strategies for the weights.</p><p>We know now that the weights should be initialized in a way that they differ from each other to ensure symmetry breaking, i.e. preventing the neurons from evolving identically. One way to achieve this is random initialization. However, immediately the next question arises: How to generate those random values? For example, weights can be drawn from a Gaussian distribution with mean zero and some fixed standard deviation. Choosing a small standard deviation, e.g. 0.01, may cause a problem known as vanishing gradients for deep networks, since the small neuron values will be multiplied with each other in the computation of gradients due to the chain rule, leading to exponentially decaying products. As a result learning can be very slow or even diverge. On the other hand, choosing a large standard deviation, e.g. 0.2, can result in exploding gradients, which is essentially the opposite problem, where the products grow exponentially and learning can become unstable, oscillate or even produce "NaN" values for the variables. Furthermore, in combination with saturating activation functions like sigmoid or tanh exploding variable values can lead to saturation of the activation function, which then leads to vanishing gradients and again hinder learning, cf. Figure <ref type="figure" target="#fig_1">3</ref>.</p><p>To find a good intermediate value for the standard deviation, Xavier initialization has been proposed in <ref type="bibr" target="#b13">[14]</ref>, where the standard deviation value is chosen depending on the input size of the layer, i.e.</p><formula xml:id="formula_37">1 √ n for W [ ] , = 0, . . . , L -1.</formula><p>Note that since input sizes can vary, the Gaussian distribution that the initial values are drawn from, will also vary. This choice of weights in combination with b [ ] = 0 leads to Var(y [ +1] ) = Var(y [ ] ). However, the Xavier initialization assumes zero centered activation functions, which is not fulfilled for sigmoid and all variants of ReLU. As a remedy, the He initialization has been proposed in <ref type="bibr" target="#b17">[18]</ref>, tailored especially to ReLU activation functions.</p><p>Here, the standard deviation is also chosen depending on the input size of the layer, namely</p><formula xml:id="formula_38">2 n .</formula><p>Additionally, there also exists an approach to normalize feature vectors throughout the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Batch Normalization</head><p>Assume that we are employing an optimization algorithm, which passes through the data points in batches of size b and that the nodes in hidden layers follow a normal distribution. Then batch normalization <ref type="bibr" target="#b21">[22]</ref> aims at normalizing the feature vectors in a hidden layer over the given batch, to stabilize training, especially for unbounded activation functions, such as ReLU.</p><p>It can be seen as insertion of an additional layer in a deep neural network, and this layer type is already pre-implemented in learning frameworks like tensorflow and pytorch:</p><p>• tf.keras.layers.BatchNormalization,</p><p>• torch.nn.BatchNorm1d (also 2d and 3d available).</p><p>Say, we have given the current feature vectors y [ ] j ∈ R n of hidden layer for all elements in the batch, i.e. j = 1, . . . , b. The batch normalzation technique first determines the mean and variance</p><formula xml:id="formula_39">µ [ ] = 1 b b j=1 y [ ] j , (σ 2 ) [ ] = 1 b b j=1 y [ ] j -µ [ ] 2 .</formula><p>Subsequently, the current feature vectors y</p><formula xml:id="formula_40">[ ] j , j = 1, . . . , b are normalized via ŷ[ ] j = y [ ] j -µ [ ] (σ 2 ) [ ] + ,</formula><p>where ∈ R is a constant that helps with numerical stability. Finally, the output of the batch normalization layer is computed by</p><formula xml:id="formula_41">y [ +1] j = W [ ] ŷ[ ] j + b [ ] ∀ j = 1, . . . , b.</formula><p>As usual, the weight W [ ] ∈ R n +1 ×n and bias b [ ] ∈ R n +1 are variables of the neural network and will be learned.</p><p>Figure <ref type="figure" target="#fig_0">12</ref>. Illustration of Batch Normalization applied to a hidden layer with 3 nodes and batch size b. We assume that every node can be modeled by a normal distribution. Image Source: https://towardsdatascience.com/batch-normalization-in-3-levels-ofunderstanding-14c2da90a338.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Remark 2.2</head><p>The BN layer is a trainable layer, since it contains learnable variables.</p><p>Let us now consider an example task that is commonly solved with FNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Classification Tasks</head><p>As mentioned in 1.1, classification is a supervised learning task with labels S(u) ∈ N. First, we consider binary classification, cf. e.g. [12, Section 2.1], where we classify the data into two categories, i.e. a spam filter that determines whether an email is spam or not. We could construct our network so that it indicates the category which it concludes is the most likely. However, it can be useful to know the probability assigned to the output, so that we know how certain the decision is. Thus, we aim to have outputs between 0 and 1, so that they sum up to 1. In the case of binary classification, the second output is directly determined by the first. Consequently, it suffices to have a one dimensional output layer y [L] ∈ [0, 1], which should predict</p><formula xml:id="formula_42">P (y [L] = 1 | u, θ),</formula><p>i.e. the probability of the output being category 1 (e.g. spam) given the input u and variables θ.</p><p>Assume that we have already set up a feedforward network up to the second to last layer y [L-1] ∈ R. It remains to choose the activation function σ [L-1] that enters the computation of y [L] and to model the loss function L .</p><p>Since we want y [L] ∈ [0, 1], a common approach is to use the sigmoid activation function.</p><formula xml:id="formula_43">σ(y) = 1 1 + exp(-y) = exp(y) exp(y) + 1 ∈ (0, 1). -4 -2 0 2 4 0 0.5<label>1</label></formula><p>Let us remark that the cases y [L] ∈ {0, 1} are not possible with this choice of activation function, but we are only computing approximations anyhow.</p><p>Next, we construct a loss function. To this end, we assume that the training data is a sample of the actual relationship we are trying to train, thus it obeys a probability function, which we want to recover. The main idea for the loss function is to maximize the likelihood of the input parameters, i.e. if the probability P (y [L] = S(u) | u, θ) of generating the known supervision S(u) is high for the input data u, the loss should be small, and vice versa. To model the probability we choose the Bernoulli distribution, which models binary classification:</p><formula xml:id="formula_44">P y [L] = S(u) | u, θ = (y [L] ) S(u) (1 -y [L] ) (1-S(u)) .</formula><p>To achieve small loss for large probabilities, we apply the logarithm and then maximize this function, so that the optimal network variables θ can be determined as follows</p><formula xml:id="formula_45">θ = argmax θ N i=1 log P y [L](i) = S(u (i) ) | u (i) , θ = argmax θ N i=1 log (y [L](i) ) S(u (i) ) (1 -y [L](i) ) (1-S(u (i) )) = argmax θ N i=1 S(u (i) ) • log(y [L](i) ) + (1 -S(u (i) )) • log(1 -y [L](i) ) = argmin θ N i=1 -S(u (i) ) • log(y [L](i) ) -(1 -S(u (i) )) • log(1 -y [L](i) ) =:L(y [L](i) ,S(u (i) )), Binary Cross Entropy Loss</formula><p>, where y [L](i) is a function of the network variables θ.</p><p>In practice, we minimize the cross-entropy, since it is equivalent to maximizing the likelihood, but stays within our given frame of minimization problems. Let us assume that our data either has the label S(u) = 1 (spam) or S(u) = 0 (not spam), then the binary cross entropy loss is We see in Figure <ref type="figure" target="#fig_10">13</ref> that the cross entropy loss for S(u) = 1 goes to zero, for y [L]  1, and grows for y [L]  0, as desired. On the other hand, the cross entropy loss for S(u) = 0 goes to zero for y [L]  0 and grows for y [L]  1.</p><formula xml:id="formula_46">L y [L] , S(u) = -log(y [L] ), if S(u) = 1, -log(1 -y [L] ), if S(u) = 0.</formula><p>Altogether, we have the following loss function for the binary classification task</p><formula xml:id="formula_47">L (θ) = 1 N N i=1 L y [L](i) (θ), S(u (i) ) .</formula><p>Multiclass classification is a direct extension of binary classification. Here, the goal is to classify the data into multiple (at least 3) categories. A prominent example is the MNIST data set, where black and white pictures of handwritten digits (of size 28 × 28 pixels) are supposed to be classified as {0, 1, 2, 3, 4, 5, 6, 7, 8, 9}, cf. Figure <ref type="figure" target="#fig_11">14</ref>. A possible network architecture is illustrated in Figure <ref type="figure" target="#fig_12">15</ref>. . . .</p><formula xml:id="formula_48">y [1] 1 y [1] 2 y [1] 3 y [1] n1 . . . y [2] 1 y [2] 2 y [2] n2</formula><p>. . .</p><formula xml:id="formula_49">y [3] 1 y [3]<label>n3</label></formula><p>. . . For this task, we need a generalization of the sigmoid activation function, which will take y</p><formula xml:id="formula_50">[L-1] ∈ R n and map it to y [L] ∈ [0, 1] n , so that we have n i=1 y [L] i = 1</formula><p>, where n L-1 = n L = n is the number of classes. A suitable option is the softmax function, which is given component-wise by</p><formula xml:id="formula_51">softmax(y) i = exp(y i ) n j=1 exp(y j ) ∈ (0, 1), for i = 1, . . . , n.</formula><p>Keep in mind that e.g. in the MNIST case we have labels S(u) ∈ {0, 1, 2, 3, 4, 5, 6, 7, 8, 9}, and in general we have multiple labels S(u) ∈ N. We have seen before that maximizing the loglikelihood is a suitable choice for classification tasks. Since we want to formulate a minimization problem, we choose the negative log-likelihood as loss function</p><formula xml:id="formula_52">L (θ) = 1 N N 1=1 -log P y [L](i) = S(u (i) ) | u (i) , θ .</formula><p>In this section we have seen yet another model for the loss function L , and from Section 1.3 we know that in any case we will need the gradient ∇L (θ) to update our variables θ. Let us discuss how frameworks like pytorch and tensorflow obtain this information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">Backpropagation</head><p>The derivations are based on [15, Section 6.5] and <ref type="bibr" target="#b25">[26,</ref><ref type="bibr">Section 7.3</ref>]. When a network, e.g. a FNN, takes an input u, passes it through its layers and finally computes an output y [L] , the network feeds forward the information, which this is called forward propagation. Then a loss is assigned to the output and we aim at employing the gradient of the loss function ∇L (θ) to update the network variables θ ∈ R K . In a FNN we have</p><formula xml:id="formula_53">K = n 0 •n 1 +. . .+n L-1 •n L +n 1 +. . .+n L .</formula><p>The gradient is then given by</p><formula xml:id="formula_54">∇L (θ) =     ∂L (θ) ∂θ1 . . . ∂L (θ) ∂θ K     .</formula><p>To develop an intuition about the process, we discuss the following simple example.</p><formula xml:id="formula_55">y [0]</formula><p>y [1]  y [2]  y [3]  input layer hidden layers L output layer</p><formula xml:id="formula_56">W [0]</formula><p>W [1]  W [2]   Figure <ref type="figure" target="#fig_6">16</ref>. A feedforward network with 2 hidden layers, one node per layer and depth L = 3.</p><p>Example 2.3 Consider a very simple FNN with one node per layer and assume that we only consider weights W [ ] ∈ R and no biases. For the network in Figure <ref type="figure" target="#fig_6">16</ref> we have θ = (W [0] , W [1] , W [2] ) , y [3] = σ [3] (W [2] y [2] ) = σ [3] W [2] σ [2] (W [1] y [1] ) = σ [3] W [2] σ [2] W [1] σ [1] (W [0] y [0] ) , and L (θ) = L (y [3] (θ)) = L σ [3] W [2] σ [2] W [1] σ [1] (W [0] y [0] ) .</p><p>Computing the components of the gradient, we employ the chain rule to obtain e.g.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>∂L ∂W</head><p>[0] = ∂L ∂y [3] • ∂y [3]  ∂W [0]   = ∂L ∂y [3] • ∂y [3]  ∂y [2] • ∂y [2]  ∂W [0]   = ∂L ∂y [3] • ∂y [3]  ∂y [2] • ∂y [2]  ∂y [1] • ∂y [1]  ∂W [0] ,</p><p>and in general for depth L we get</p><formula xml:id="formula_57">∂L ∂W [ ] = ∂L ∂y [L] • +2 j=L ∂y [j] ∂y [j-1] • ∂y [ +1] ∂W [ ] .</formula><p>Essentially, to calculate the effect of a variable on the loss function we iterate backwards through the network, multiplying the derivatives of each layer. This is called back propagation, often abbreviated as backprop.</p><p>To obtain a computationally efficient version of back propagation, we exploit the fact that parts of the derivatives can be recycled, broadly speaking. E.g. ∂L ∂y [L] is a part of all derivatives. So, if we compute the derivative by W [L-1] first, we already have this component available and can reuse it in the computation of the derivative by W [L-2] , etc.</p><p>In order to formalize the effective computation of derivatives in a backpropagation algorithm, we decompose the forward propagation into two parts, cf. e.g. <ref type="bibr" target="#b25">[26,</ref><ref type="bibr">Section 7.3.2]</ref>.</p><formula xml:id="formula_58">z [ ] = W [ -1] y [ -1] + b [ -1] ∈ R n , y [ ] = σ ] (z [ ] ) ∈ R n .</formula><p>This was not necessary in Example 2.3, because we only consider weights and no biases. Furthermore, we assume that the loss function L takes the final output y [L] as an input. Especially, no other feature vectors y [ ] for = L enter the loss function directly. This is the case e.g. for mean squared error, cf. Example 1.2, and cross entropy, cf. Section 2.4.</p><p>In general, we now have by chain rule for all = 0, . . . , L -1</p><formula xml:id="formula_59">∂L ∂W [ ] = ∂L ∂y [L] • +2 j=L ∂y[j] ∂z [j] • ∂z [j] ∂y [j-1] • ∂y [ +1] ∂z [ +1] • ∂z [ +1] ∂W [ ] ,<label>(3)</label></formula><formula xml:id="formula_60">∂L ∂b [ ] = ∂L ∂y [L] • +2 j=L ∂y[j] ∂z [j] • ∂z [j] ∂y [j-1] • ∂y [ +1] ∂z [ +1] • ∂z [ +1] ∂b [ ] .<label>(4)</label></formula><p>However, we have to understand these derivatives in detail. First of all, let us introduce the following definition from <ref type="bibr" target="#b20">[21]</ref>.</p><p>Definition 2.4 Let A, B ∈ R m×n be given matrices, then A B ∈ R m×n , with entries</p><formula xml:id="formula_61">(A B) i,j := (A) i,j • (B) i,j , for i = 1, . . . , m, j = 1, . . . , n,</formula><p>is called the Hadamard product of A and B.</p><p>Furthermore, we define the derivative of the component-wise activation function σ : R m → R m as follows</p><formula xml:id="formula_62">σ : R m → R m , z =    z 1 . . . z m    →    σ (z 1 ) . . . σ (z m )    = σ (z).</formula><p>Let us introduce two special cases of multi-dimensional chain rule, cf. [26, p.98], which will prove helpful to calculate the derivatives.</p><p>1. Consider a = σ(z) ∈ R m , where σ is a component-wise function, e.g. an activation function, z ∈ R m , and f = f (a) ∈ R. Then, it holds</p><formula xml:id="formula_63">∂f ∂z = ∂f ∂a σ (z) ∈ R m .<label>(5) 2</label></formula><formula xml:id="formula_64">. Consider z = W y + b ∈ R m and f = f (z) ∈ R, with W ∈ R m×n and y ∈ R n . Then, it holds ∂f ∂y = W • ∂f ∂z ∈ R n ,<label>(6)</label></formula><formula xml:id="formula_65">∂f ∂W = ∂f ∂z • y ∈ R m×n ,<label>(7)</label></formula><formula xml:id="formula_66">∂f ∂b = ∂f ∂z ∈ R m .<label>(8)</label></formula><p>We can now start working our way backwards through the network to get all derivatives. Assume that we know ∂L ∂y [L] ∈ R n L , which will depend in detail on the choice of loss function. We can employ <ref type="bibr" target="#b4">(5)</ref> </p><formula xml:id="formula_67">with f = L , z = z [L] , a = y [L] to compute z[L] := ∂L ∂z [L] = ∂L ∂y [L] (σ [L] ) (z [L] ) ∈ R n L .</formula><p>Here, we employ a typical notation from automatic differentiation (AD), i.e. the gradient of the loss with respect to a certain variable is denoted by the name of that variable with an overbar. Now, we know</p><formula xml:id="formula_68">ȳ[L-1] := ∂L ∂y [L-1] = z[L] • ∂z [L] ∂y [L-1] ∈ R n L-1 ,</formula><p>i.e. we can reuse the previously derived gradient. Furthermore, from <ref type="bibr" target="#b5">(6)</ref> </p><formula xml:id="formula_69">with f = L , y = y [L-1] , W = W [L-1] , z = z [L] we deduce ȳ[L-1] = (W [L-1] ) z[L] .</formula><p>Subsequently, we use ȳ[L-1] to compute z[L-1] , and so forth. In this way we can keep iterating to build up the products in ( <ref type="formula" target="#formula_36">3</ref>) and ( <ref type="formula" target="#formula_60">4</ref>).</p><p>In every layer, = 0, . . . , L -1, we also want to determine</p><formula xml:id="formula_70">W [ ] := ∂L ∂W [ ] and b[ ] := ∂L ∂b [ ] .</formula><p>We show this exemplary for = L -1. It holds</p><formula xml:id="formula_71">W [L-1] = z[L] • ∂z [L] ∂W [L-1] ∈ R n L ×n L-1 , b[L-1] = z[L] • ∂z [L] ∂b [L-1] ∈ R n L .</formula><p>Making use of ( <ref type="formula" target="#formula_65">7</ref>) and ( <ref type="formula" target="#formula_66">8</ref>) with the same choices as in the computation of ȳ[L-1] and b = b [L-1] , we get</p><formula xml:id="formula_72">W [L-1] = z[L] (y [L-1] ) , b[L-1] = z[L] .</formula><p>With this technique, we have an iterative way to efficiently calculate all gradients needed for the variable update in the optimization method, cf. Section 1.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Remark 2.5</head><p>(i) It is even more elegant and efficient to update W [ ] and b [ ] during backpropagation, i.e. on the fly. This way we do not need to store the gradient and can overwrite the weight and bias variables. It is only necessary to save the current gradients for the next loop, so we could rewrite the backpropagation algorithm with temporary gradient values. This only works if the stepsize / learning rate τ is previously known and fixed for all variables, since for a line search we would need to know the full gradient and could only update the variables afterwards.</p><p>(ii) Considering we have N training data points, the backpropagation algorithm has to take all of them into account. When the loss is a sum of loss functions for each data point, this can be easily incorporated into the algorithm by looping over i = 1, . . . , N and introducing a sum where necessary.</p><p>Altogether, we formulate Algorithm 6, which collects the gradients with respect to the weights and biases in one final gradient vector ∇L (θ).</p><p>In frameworks like pytorch and tensorflow, backpropagation is already implemented, e.g. in pytorch the function "autograd" handles the backward pass. Broadly speaking, autograd collects Algorithm 6 Backpropagation.</p><p>Require: Training data set {u (i) , S(u (i) )} N i=1 . Require: Current weights W [ ] and biases b [ ] for = 0, . . . , L -1.</p><p>Require: Activation functions σ [ ] for = 1, . . . , L.</p><p>Require: Loss function L (y [L] ) and its gradient ∇L (y</p><formula xml:id="formula_73">[L] ) = ∂L ∂y [L] ∈ R n L . y [0](i) = u (i) ∈ R n0 for i = 1, . . . , N . for = 1, . . . , L do z [ ](i) = W [ -1] y [ -1] + b [ -1] ∈ R n for i = 1, . . . , N , y [ ](i) = σ [ ] (z [ ](i) ) ∈ R n for i = 1, . . . , N .</formula><p>end for</p><formula xml:id="formula_74">Compute loss L = 1 N N 1=1 L (y [L](i) ) ∈ R. ȳ[L](i) = 1 N • ∇L (y [L](i) ) ∈ R n L for i = 1, . . . , N . for = L, L -1, . . . , 1 do z[ ](i) = ȳ[ ](i) (σ [ ] ) (z [ ](i) ) ∈ R n for i = 1, . . . , N , ȳ[ -1](i) = (W [ -1] ) z[ ](i) ∈ R n -1 for i = 1, . . . , N , W [ -1] = N 1=1 z[ ](i) (y [ -1](i) ) ∈ R n ×n -1 , b[ -1] = N 1=1 z[ ](i) ∈ R n .</formula><p>end for the data and all executed operations in a directed acyclic graph. In this graph the inputs are the leaves, while the outputs are the roots. Now to automatically compute the gradients, the graph can be traced from roots to leaves, employing the chain rule. This coincides with the computations that we just derived by hand. For more details we refer to https://pytorch.org/ tutorials/beginner/blitz/autograd_tutorial.html.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Convolutional Neural Network</head><p>In this section, based on <ref type="bibr" target="#b8">[9]</ref>,[15, Section 9], we consider Neural Networks with a different architecture: convolutional neural networks (CNNs or ConvNets). They were first introduced by Kunihiko Fukushima in 1980 under the name "neocognitron" <ref type="bibr" target="#b10">[11]</ref>. Famous examples of convolutional neural networks today are "LeNet" <ref type="bibr" target="#b24">[25]</ref>, see Figure <ref type="figure" target="#fig_13">17</ref> and "AlexNet" <ref type="bibr" target="#b23">[24]</ref>.</p><p>As a motivation, consider a classification task where the input is an image of size n 0,1 × n 0,2 pixels. We want to train a Neural Network so that it can decide, e.g. which digit is written in the image (MNIST data set). We have seen in Figure <ref type="figure" target="#fig_12">15</ref> that the image with n 0,1 = n 0,2 = 28 has been reshaped (vectorized, flattened) into a vector in R n0,1•n0,2 = R 784 , so that we can use it as an input for a regular FNN. However, this approach has several disadvantages:</p><p>1. Vectorization causes the input image to loose all of its spatial structure, which could have been helpful during training.</p><p>2. Let e.g. n 0,1 = n 0,2 = 1000, then n 0 = 10 6 and the weight matrix W [0] ∈ R n1×10 6 contains an enormous number of optimization variables. This can make training very slow or even infeasible.</p><p>On the contrary, convolutional neural networks are designed to exploit the relationships between neighboring pixels. In fact, the input of a CNN is typically a matrix or even a three-dimensional tensor, which is then passed through the layers while maintaining this structure. CNNs take small patches, e.g. squares or cubes, from the input images and learn features from them. Consequently, they can subsequently recognize these features in other images, even when they appear in other parts of the image. In Figure <ref type="figure" target="#fig_13">17</ref> we see the architecture of "LeNet-5". The inputs are images, where we have 1 channel, because we consider grayscale images. At first we have two sequences of convolution layer (yellow), Section 3.2, detector layer (violet), Section 3.3, and pooling layer (orange), Section 3.4. These layers retain the multidimensional structure of the input. Since this network is built for a classification tasks, the output should be a vector of 10. Consequently, the multi-dimensional output of a hidden layer is flattened, i.e. vectorized, and the remaining layers are fully connected layers (bright yellow) as we have seen in FNNs.</p><p>In other, larger architectures, like AlexNet, cf. Figure <ref type="figure" target="#fig_21">24</ref>, to avoid overfitting with large fully connected layers, a technique called dropout is applied. The key idea is to randomly drop units with a given probability and their connections from the neural network during training, for more details we refer to <ref type="bibr" target="#b27">[28]</ref>.</p><p>Remark 3.1</p><p>(i) We will view convolution, detector and pooling layers as separate layers. However, it is also possible to define a convolutional layer to consist of a convolution, detector and pooling stage, cf. Figure <ref type="figure" target="#fig_8">18</ref>. This can be a source of confusion when referring to convolutional layers, which we should be aware of.</p><p>(ii) Throughout the remainder of this section we omit layer indices to simplify notation, and we indicate the data with capital letter Y to clarify that they are matrices or tensors.</p><p>Figure <ref type="figure" target="#fig_8">18</ref>.</p><p>Convolutional layer (gray) consisting of stages (left) compared to viewing the operations as separate layers (right). We use the terminology as depicted on the right hand side, and refer to convolutional, detector and pooling layers as separate layers.</p><p>Before we move on to a detailed introduction of the different layer types in CNNs, let us recall the mathematical concept of a convolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Convolution</head><p>As explained in [15, Section 9.1], in general, convolution describes how one function influences the shape of another function. But it can also be used to apply a weight function to another function, which is how convolution is used in convolutional neural networks. Definition 3.2 Let f, g : R n → R be two functions. If both f and g are integrable with respect to Lebesgue measure, we can define the convolution as:</p><formula xml:id="formula_75">c(t) = (f * g)(t) = f (x)g(t -x) dx,</formula><p>for some t ∈ R n . Here, f is called the input and g is called the kernel. The new function c : R n → R is called the feature map.</p><p>However, for convolutional neural networks we need the discrete version. Definition 3.3 Let f, g : Z n → R be two discrete functions. The discrete convolution is then defined as:</p><formula xml:id="formula_76">c(t) = (f * g)(t) = x∈Z n f (x)g(t -x),</formula><p>for some t ∈ Z n .</p><p>A special case of the discrete convolution is setting f and g to n-dimensional vectors and using the indices as arguments. We illustrate this approach in the following example.</p><p>Example 3.4 Let X and Y be two random variable each describing the outcome of rolling a dice. The probability mass functions are defined as:</p><formula xml:id="formula_77">f X (t) = f Y (t) = 1 6 , if t ∈ {1, 2, 3, 4, 5, 6}, 0, if t ∈ Z \ {1, 2, 3, 4, 5, 6}.</formula><p>We aim at calculating the probability that the sum of both dice rolls equals nine. To this end, we take the vectors of all possible outcomes and arrange them into two rows. Here, we flip the second vector and slide it to the right, such that the numbers which add to nine align. Now, we replace the outcomes with their respective probabilities, multiply the adjacent components and add up the results. This gives</p><formula xml:id="formula_78">f X+Y (9) = 1 36 + 1 36 + 1 36 + 1 36 = 1 9 ,</formula><p>i.e. the probability that the sum of the dice equals nine is 1 9 . In fact, all the steps we have just done are equivalent to calculating a discrete convolution:</p><formula xml:id="formula_79">f X+Y (9) = 6 x=1 f X (x)f Y (9 -x) = (f X * f Y )(9)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Convolutional Layer</head><p>For the convolutional layers in CNNs we define convolutions for matrices, cf. e.g. <ref type="bibr">[15, (9.4)</ref>]. This can be extended to tensors straight forward. Definition 3.5 Let Y ∈ R n1×n2 and K ∈ R m1×m2 be given matrices, such that m 1 ≤ n 1 and m 2 ≤ n 2 . The convolution of Y and K is denoted by Y * K with entries</p><formula xml:id="formula_80">[Y * K] i,j := m1 k=1 m2 l=1 K k,l Y i+m1-k,j+m2-l , for 1 ≤ i ≤ n 1 -m 1 + 1 and 1 ≤ j ≤ n 2 -m 2 + 1.</formula><p>Here, Y is called the input and K is called the kernel.</p><p>In Machine Learning often the closely related concept of (cross) correlation, cf. e.g. <ref type="bibr">[15, (9.6)</ref>], is used, and incorrectly referred to as convolution, where</p><formula xml:id="formula_81">[Y K] i,j := m1 k=1 m2 l=1 K k,l Y i-1+k,j-1+l , for 1 ≤ i ≤ n 1 -m 1 + 1 and 1 ≤ j ≤ n 2 -m 2 + 1.</formula><p>The (cross) correlation has the same effect as convolution, if you flip both, rows and columns of the kernel K, see the changed indices indicated in red. Since we learn the kernel anyway, it is irrelevant whether the kernel is flipped, thus either concept can be used.</p><p>We illustrate the matrix computations with an example. The computation of [Y * K] 1,1 can be illustrated as follows</p><formula xml:id="formula_82">[Y * K] 1,1 =     +1 • 9 +5 • 8 -2 • 7 0 2 +3 • 6 +8 • 5 +7 • 4 1 0 -1 • 3 +0 • 2 +1 • 1 2 3 4 2 1 -1 2     = 9 + 40 -14 + 18 + 40 + 28 -3 + 0 + 1 = 119.</formula><p>The gray values of Y are not used in the computation. Here, we see that K is flipped when used in the convolution. This also clarifies, how the (cross) correlation can be more intuitive, where</p><formula xml:id="formula_83">[Y K] 1,1 =     +1 • 1 +5 • 2 -2 • 3 0 2 +3 • 4 +8 • 5 +7 • 6 1 0 -1 • 7 +0 • 8 +1 • 9 2 3 4 2 1 -1 2     = 1 + 10 -6 + 12 + 40 + 42 -7 + 0 + 9 = 101.</formula><p>In a similar way we can proceed to calculate the remaining values by shifting the kernel over the matrix  The kernel size, which is typically square, e.g. m × m, is a hyperparameter of the CNN. Furthermore, the convolutional layer has additional hyperparameters that need to be chosen. We have seen that a convolution with a m × m kernel reduces the dimension from</p><formula xml:id="formula_84">[Y K] 1,2 =     1 +5 • 1 -2 • 2 +0 • 3 2 3 +8 • 4 +7 • 5 +1 • 6 0 -1 +0 • 7 +1 • 8 +2 • 9 3 4 2 1 -1 2     =</formula><formula xml:id="formula_85">n 1 × n 2 to n 1 -m + 1 × n 2 -m + 1.</formula><p>To retain the image dimension we can use (zero) padding, cf. <ref type="bibr" target="#b8">[9]</ref>, applied to the input Y with p ∈ N 0 . Choosing p = 0 yields Y again, whereas p = 1 results in</p><formula xml:id="formula_86">Ŷ =         0 0 0 0 0 0 0 0 1 5 -2 0 2 0 0 3 8 7 1 0 0 0 -1 0 1 2 3 0 0 4 2 1 -1 2 0 0 0 0 0 0 0 0        </formula><p>, for Y from Example 3.6. Consequently, the padded matrix Ŷ is of dimension (n 1 +2p)×(n 2 +2p).</p><p>To retain the image dimension, we need to choose p so that</p><formula xml:id="formula_87">(n 1 + 2p) -m + 1 = n 1 , (n 2 + 2p) -m + 1 = n 2 ,</formula><p>i.e. p = m-1 2 , which is possible for any odd m. Furthermore, we can choose the stride s ∈ N, which indicates how far to move the kernel. For example, in Figure <ref type="figure" target="#fig_18">19</ref> the stride is chosen as s = 1, while the stride in Figure <ref type="figure" target="#fig_19">20</ref> </p><formula xml:id="formula_88">is s = 2.</formula><p>Let us remark that a stride s &gt; 1 reduces the output dimension of the convolution to</p><formula xml:id="formula_89">n 1 -m s + 1 × n 2 -m s + 1 .</formula><p>Altogether, we can describe the convolutional layer. It consists of M ∈ N filters with identical hyperparameters: kernel size m×m, padding p and stride s, but each of them has its own learnable kernel K. Consequently, the filters in this layer have M • m 2 variables in total. Applying all M filters to an input matrix Y ∈ R n1×n2 leads to an output of size</p><formula xml:id="formula_90">n 1 + 2p -m s + 1 × n 2 + 2p -m s + 1 × M,</formula><p>where the results for all M filters are stacked, cf. <ref type="bibr" target="#b8">[9]</ref>. Typically, the depth M is chosen as a power of 2, and growing for deeper layers, while height and width are shrinking, cf. Figure <ref type="figure" target="#fig_21">24</ref>.</p><p>Obviously, the output is a tensor with three dimensions, hence the subsequent layers need to process 3-tensor-valued data. In fact, for colored images already the original input of the network is a tensor. The (cross) correlation operation (and also the convolution operation) can be generalized to this case in the following way.</p><p>Assume we have an input tensor of size n 1 × n 2 × n 3 , then we choose a three dimensional kernel of size m × m × n 3 , i.e. the depth coincides. No striding or padding is applied in the third dimension. Hence, the output is of dimension</p><formula xml:id="formula_91">n 1 + 2p -m s + 1 × n 2 + 2p -m s + 1 × 1,</formula><p>which can be understood as a matrix by discarding the redundant third dimension, cf. Figure <ref type="figure" target="#fig_0">21</ref>. Doing this for M filters, again leads to the output being a 3-tensor.</p><p>Figure <ref type="figure" target="#fig_0">21</ref>. Illustration of a convolution on a tensor, specifically a colored image (with red, green, blue color channels) with a three-dimensional kernel and its result, which is a matrix. Here, no padding p = 0 and a single stride s = 1 is employed. Image Source: https://datahacker.rs/ convolution-rgb-image/.</p><p>Remark 3.7</p><p>(i) Convolutional layers have the advantage that they have less variables than fully connected layers applied to the flattened image. For example consider a grayscale image of size 28×28 as input in LeNet, cf. Figure <ref type="figure" target="#fig_13">17</ref>. The first convolution has 6 kernels with 5×5 entries. Due to padding with p = 2 the output is 28 × 28 × 6, so the image size is retained. Additionally, before applying a detector layer, we add a bias per channel, so 6 bias variables in this case.</p><p>In total, we have to learn 156 variables. Now, imagine this image is flattened to a 784 × 1 vector and fed into a FNN with fully connected layer, where we also want to retain the size, i.e. the first hidden layer has 784 nodes. This results in a much larger number of variables:</p><formula xml:id="formula_92">784 • 784 weight + 784 bias = 615440.</formula><p>(ii) Directly related, a disadvantage of convolutional layers is that every output only sees a subset of all input neurons, cf. e.g. [15, Section 9.2]. We denote this set of seen inputs by effective receptive field of the neuron. In an FNN with fully connected layers the effective receptive field of a neuron is the entire input. However, the receptive field of a neuron increases with depth of the network, as illustrated in Figure <ref type="figure">22</ref>.</p><formula xml:id="formula_93">y [<label>0] 1 y [0] 2 y [0] 3 y [0] 4 y [0] 5 y [1] 1 y [1] 2 y [1] 3 y [1] 4 y [1] 5 y [2] 1 y [2] 2 y [2] 3 y [2] 4 y [2] 5</label></formula><p>Figure <ref type="figure">22</ref>. Simplified CNN architecture with an input layer y [0] and two subsequent convolutional layers y [1] and y [2] , each with a one dimensional kernel of size m = 3, stride s = 1 and zero padding p = 1. The colored nodes are the receptive field of the neuron y</p><p>[2] 3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Detector Layer</head><p>In standard CNN architecture after a convolutional layer, a detector layer is applied. This simply means performing an activation function. To this end, we extend the activation function σ : R → R to matrix and tensor valued inputs by applying it component-wise, as we did for vectors before in Section 2, e.g. for a 3-tensor</p><formula xml:id="formula_94">Y = {Y i,j,k } i,j,k with i = 1, . . . , n 1 , j = 1, . . . , n 2 , k = 1, . . . , n 3 we get (σ(Y )) i,j,k = σ(Y i,j,k ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Pooling Layer</head><p>After the detector layer, typically a pooling layer (also called downsampling layer) is applied, cf. e.g. <ref type="bibr" target="#b14">[15,</ref><ref type="bibr">Section 9.3]</ref>. This layer type is responsible for reducing the first two dimensions (height and width) and usually does not interfere with the third dimension (depth) of the data Y , but rather is applied for all channels independently. Consequently, the depth of the output coincides with the depth of the input and we omit the depth in our discussion.</p><p>As in convolutional layers, pooling layers have a filter size m × m, stride s and padding p. However, almost always p = 0 is chosen. The most popular values for for the filter size and stride are m = s = 2. Again, with an input of size n 1 × n 2 the output dimension is</p><formula xml:id="formula_95">n 1 + 2p -m s + 1 × n 1 + 2p -m s + 1 m=s=2,p=0 = n 1 2 × n 2 2 .</formula><p>One common choice is Max Pooling (or Maximum Pooling), where the largest value is selected, cf. e.g. <ref type="bibr" target="#b8">[9]</ref>. Below we see an example of max pooling with a 2 × 2 kernel, stride s = 2 and no padding.</p><formula xml:id="formula_96">    1 3 0 -7 -2 4 1 -1 0 1 8 -3 2 0 4 5     max -→ 4 1 2 8</formula><p>Another common choice is Average Pooling, where we take the mean of all values. Below we see an example of average pooling (abbreviated: "avg") with a 2 × 2 kernel, K = 0.25 0.25 0.25 0.25 , stride s = 2 and no padding.</p><formula xml:id="formula_97">    1 3 0 -7 -2 4 1 -1 0 1 8 -3 2 0 4 5     avg -→ 1.50 -1.75 0.75 3.50</formula><p>The effect of average pooling applied to an image is easily visible: It blurs the image. In the new image every pixel is an average of a pixel and its neighboring m 2 -1 pixels, see Figure <ref type="figure" target="#fig_20">23</ref>.</p><p>Depending on the choice of stride s and padding p, the blurred image may also have less pixels. (ii) We have seen that when using CNNs, we make the following assumptions:</p><p>(a) Pixels far away from each other do not need to interact with each other.</p><p>(b) Small translations are not relevant.</p><p>If these assumptions do not hold, employing a CNN can result in underfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Local Response Normalization</head><p>Similar to batch normalization, Local Response Normalization (LRN) [24, Section 3.3] stabilizes training with unbounded activation functions like ReLU. This strategy was first introduced within the "AlexNet" architecture <ref type="bibr" target="#b23">[24]</ref>, cf. Figure <ref type="figure" target="#fig_21">24</ref>, because contrary to previous CNNs like "LeNet-5", which used sigmoid activation, "AlexNet" employs ReLU activation. The inter-channel LRN, as introduced in [24, Section 3.3], see also Figure <ref type="figure" target="#fig_22">25 a</ref>), is given by</p><formula xml:id="formula_98">Ŷi,j,k = Y i,j,k κ + γ min(M,k+ n 2 ) m=max(1,k-n 2 ) (Y i,j,m ) 2 β .</formula><p>Here, Y i,j,k and Ŷi,j,k denote the activity of the neuron before and after normalization, respectively. The indices i, j, k indicate the height, width and depth of Y . We have i = 1, . . . , n 1 , j = 1, . . . , n 2 and k = 1, . . . , M , where M is the number of filters in the previous convolutional layer. The values κ, γ, β, n ∈ R are hyperparameters, where κ is used to avoid singularities, and γ and β are called normalization and contrasting constants, respectively. Furthermore, n dictates how many surrounding neurons are taken into consideration, see also Figure <ref type="figure" target="#fig_22">25</ref>. In <ref type="bibr" target="#b23">[24]</ref> κ = 2, γ = 10 -4 , β = 0.75 were chosen. In the case of intra-channel LRN, the neighborhood is extended within the same channel. This leads to the following formula</p><formula xml:id="formula_99">Ŷi,j,k = Y i,j,k κ + γ min(n1,i+ n 2 ) p=max(1,i-n 2 ) min(n2,j+ n 2 ) q=max(1,j-n 2 ) (Y p,q,k ) 2 β .</formula><p>Remark 3.9 The LRN layer is non-trainable, since it only contains hyperparameters and no variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">ResNet</head><p>We have seen in Example 2.3 that for a FNN with depth L we have the derivative</p><formula xml:id="formula_100">∂L ∂W [ ] = ∂L ∂y [L] • +2 j=L ∂y [j] ∂y [j-1] • ∂y [ +1] ∂W [ ] .</formula><p>In the case that we consider a very deep network, i.e. large L, the product in the derivative can be problematic, <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b13">14]</ref>, especially if we take derivatives with respect to variables from early layers. Two cases may occur:</p><p>1. If ∂y [j]  ∂y [j-1] &lt; 1 for all j, the product, and hence the whole derivative, tends to zero for growing L. This problem is referred to as vanishing gradient.</p><p>2. On the other hand, if ∂y [j]  ∂y [j-1] &gt; 1 for all j, the product, and hence the whole derivative, tends to infinity for growing L. This problem is referred to as exploding gradient.</p><p>Residual Networks (ResNets) have been developed in <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b18">19]</ref> with the intention to solve the vanishing gradient problem. Employing the same notation as in FNNs, simplified ResNet layers can be represented in the following way</p><formula xml:id="formula_101">y [ ] = y [ -1] + σ [ ] (W [ -1] y [ -1] + b [ -1] ) for = 1, . . . , L,<label>(9)</label></formula><p>with y [0] = u the input data. Essentially, a ResNet is a FNN with an added skip connection, i.e. +y [ -1] , cf. Figure 26 have the same dimension n , so that we can add them up. To allow for different layer sizes, we need to insert projection operators P -1 ∈ R n ×n -1 , cf. [2, Section 4], i.e.</p><formula xml:id="formula_102">y [ -1] W [ -1] y [ -1] + b [ -1] activation σ + y [ ]</formula><formula xml:id="formula_103">y [ ] = P -1 y [ -1] + σ [ ] (W [ -1] y [ -1] + b [ -1] ) for = 1, . . . , L,</formula><p>We now revisit the simple FNN with two hidden layers from Example 2.3, and add skip connections to make it a ResNet, see Figure <ref type="figure" target="#fig_7">27</ref>.</p><p>Example 4.2 Consider a simple ResNet with one node per layer and assume that we only consider weights W [ ] ∈ R and no biases. For the network in Figure <ref type="figure" target="#fig_7">27</ref> we have θ = (W [0] , W [1] , W [2] ) , and L (θ) = L (y [3] (θ)).</p><p>We define for = 1, . . . , L a</p><formula xml:id="formula_104">[ ] := σ [ ] (W [ -1] y [ -1] ), y [0]</formula><p>y [1]  y [2]  y [3]  input layer hidden layers L output layer</p><formula xml:id="formula_105">W [0]</formula><p>W [1]  W [2]   Figure <ref type="figure" target="#fig_7">27</ref>. A ResNet with 2 hidden layers, one node per layer and depth L = 3.</p><p>so that in the ResNet setup</p><formula xml:id="formula_106">y [ ] = y [ -1] + a [ ] .</formula><p>Computing the components of the gradient, we employ the chain rule to obtain e.g.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>∂L ∂W</head><p>[0] = ∂L ∂y [3] • ∂y [3]  ∂W [0]   = ∂L ∂y [3] • ∂ ∂W [0] (y [2] + a [3] ) = ∂L ∂y [3] • ∂y [2]  ∂W [0] + ∂a [3]  ∂y [2] • ∂y [2]  ∂W [0]   = ∂L ∂y [3] • I + ∂a [3]  ∂y [2] • ∂y [2]  ∂W [0]   = ∂L ∂y [3] • I + ∂a [3]  ∂y [2] • I + ∂a [2]  ∂y [1] • ∂y [1]  ∂W [0] ,</p><p>where I denotes the identity. In general for depth L, we get</p><formula xml:id="formula_107">∂L ∂W [ ] = ∂L ∂y [L] • +2 j=L I + ∂a [j] ∂y [j-1] • ∂y [ +1] ∂W [ ] .<label>(10)</label></formula><p>If we generalize the derivative <ref type="bibr" target="#b9">(10)</ref> to ResNet architectures, where we do not only consider weights W [ ] , see e.g. [2, Theorem 6.1], the structure of the product in the derivative remains the same, i.e. it also contains an identity term.</p><p>Remember that for FNNs it holds y [j] = a [j] , i.e. the fraction in the product coincides in both cases. However, due to the added identity, even if</p><formula xml:id="formula_108">∂a [j] ∂y [j-1] &lt; 1</formula><p>holds for all j, we will not encounter vanishing gradients in the ResNet architecture. The exploding gradients problem can still occur.</p><p>We will see in Section 4.1 that there exist several versions of ResNets. However, from a mathematical point of view the simplified version ( <ref type="formula" target="#formula_101">9</ref>) is especially interesting, because it can be related to ordinary differential equations (ODEs), as first done in <ref type="bibr" target="#b15">[16]</ref>. Inserting a parameter τ [ ] ∈ R in front of the activation function σ and rearranging the terms of the forward propagation delivers</p><formula xml:id="formula_109">y [ ] = y [ -1] + τ [ ] σ(W [ -1] y [ -1] + b [ -1] ) ⇒ y [ ] -y [ -1] τ [ ] = σ(W [ -1] y [ -1] + b [ -1]</formula><p>).</p><p>Here, we consider the same activation function σ for all layers. Now, the left hand side of the equation can be interpreted as a finite difference representation of a time derivative, where τ [ ] is the time step size and y [ ] , y [ -1] are the values attained at two neighboring points in time. This relation between ResNets and ODEs is also studied under the name of Neural ODEs, <ref type="bibr" target="#b6">[7]</ref>. It is also possible to learn the time step size τ [ ] as an additional variable, <ref type="bibr" target="#b1">[2]</ref>.</p><p>Let us now introduce the different ResNet versions from the original papers, <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b18">19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Different ResNet Versions</head><p>In contrast to the simplified ResNet layer version (9) that we introduced, original ResNet architectures <ref type="bibr" target="#b16">[17]</ref> consist of residual blocks, cf. Figure <ref type="figure" target="#fig_24">28</ref>. Here, different layers are grouped together into one residual block and then residual blocks are stacked to form a ResNet. In the residual block, cf. Figure <ref type="figure" target="#fig_24">28</ref>, we have the following layer types:</p><formula xml:id="formula_110">y [ -1] Weights BN ReLU Weights BN + ReLU y [ ]</formula><p>• Weights: fully connected or convolutional layer,</p><p>• BN: Batch Normalization layer, cf. Section 2.3,</p><p>• ReLU: activation function σ = ReLU.</p><p>Clearly, the residual block and the simplified ResNet layer both contain a skip connection, which is the integral part of ResNets success, since it helps avoid the vanishing gradient problem. However, the residual block is less easy to interpret from a mathematical point of view and can not directly be related to ODEs.</p><p>In frameworks like Tensorflow and Pytorch, if you encounter a network architecture called "ResNet", it will usually be built by stacking residual blocks of this original form, Figure <ref type="figure" target="#fig_24">28</ref>.</p><p>Subsequently, in a follow up paper, <ref type="bibr" target="#b18">[19]</ref>, several other options to sort the occurring layers in a residual block have been introduced. The option which performed best in numerical tests (see also Figure <ref type="figure" target="#fig_27">30</ref>) is illustrated in Figure <ref type="figure" target="#fig_25">29</ref>. Remark 4.3 The authors of <ref type="bibr" target="#b18">[19]</ref> call the residual block in Figure <ref type="figure" target="#fig_25">29</ref> the full pre-activation residual block, since both activation functions are exercised before (pre) the skip connection. Meanwhile, in the original residual block there is also a post-activation, i.e. an activation function after (post) the skip connection. In this sense, the simplified ResNet layer (9) can be termed a pre-activation ResNet layer.</p><formula xml:id="formula_111">y [ -1] BN ReLU Weights BN ReLU Weights + y [ ]</formula><p>A ResNet built with full pre-activation residual blocks can be found e.g. in Tensorflow under the name "ResNetV2". In the literature, there also exist other variants of the simplified ResNet layer, e.g. with a weight matrix applied outside the activation function.</p><p>In Figure <ref type="figure" target="#fig_27">30</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">ResNet18</head><p>As an example for a ResNet architecture, we look at "ResNet18", cf. Figure <ref type="figure" target="#fig_28">31</ref>. Here, 18 indicates the number of layers with learnable weights, i.e. convolutional and fully connected layers. Even though the batch normalization layers also contain learnable weights, they are typically not counted here. This is a ResNet architecture intended for use on image data sets, hence the weights layers are convolutional layers, like in a CNN.</p><p>In Block A the input data is pre-processed, while Block B to Block E are built of two residual blocks each. To be certain which type of residual block the network is built of, it is recommended to look into the details of the implementation. However, in most cases the original residual block, cf. Figure <ref type="figure" target="#fig_24">28</ref>, is employed. Finally, as usual for a classification task, the data is flattened and passed through a fully connected (FC) layer before the output is generated. Altogether, "ResNet18" has over 10 million trainable parameters, i.e. variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Transfer Learning</head><p>An advantage of having so many different ResNet architectures available and also pre-trained is, that they can be employed for Transfer Learning, see e.g. <ref type="bibr" target="#b28">[29]</ref>. The main idea of transfer learning is to take a model that has been trained on a (potentially large) data set for the same type of task (e.g. image classification), and then adjust the first/last layer to fit your data set.</p><p>Depending on your task you may need to change one or both layers, for example:</p><p>(i) Your input data has a different structure: adapt the first layer.</p><p>(ii) Your data set has a different set of labels (supervisions): adapt the last layer.</p><p>If your input data and labels both coincide with the original task then you don't need to employ transfer learning. You can just use the pre-trained model for your task. When adapting a layer, this layers needs to be initialized. All remaining layers can be initialized with the pretrained weights, which will most likely give a good starting point. Then you train the adapted network on your data, which will typically take a lot loss time than training with a random initialization. Hence, transfer learning can save a significant amount of computing time. Clearly, transfer learning is also possible with other network architectures, as long as the network has been pre-trained. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Recurrent Neural Network</head><p>The Neural Networks we introduced so far rely on the assumption of independence among the training and test examples. They process one data point at a time, which is no problem for data sets, in which every data point is generated independently. However, for sequential data that occurs in machine translation, speech recognition, sentiment classification, etc., the dependence is highly relevant to the task. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Recurrent</head><formula xml:id="formula_112">u &lt;1&gt; u &lt;2&gt; u &lt;3&gt; u &lt;4&gt; u &lt;5&gt;</formula><p>to a german output sentence y, consisting of T out words y &lt;t&gt; , t = 1, . . . , T out . Hopefully, the output will be something like Heute scheint die Sonne</p><formula xml:id="formula_113">y &lt;1&gt; y &lt;2&gt; y &lt;3&gt; y &lt;4&gt;</formula><p>A comparison of FNN and RNN architecture can be seen in Figure <ref type="figure" target="#fig_1">33</ref>. For simplicity of notation we condense all hidden layers of the FNN into a representative computation node h.</p><formula xml:id="formula_114">u h y FNN u h y RNN u &lt;1&gt; h &lt;1&gt; y &lt;1&gt; u &lt;2&gt; h &lt;2&gt; y &lt;2&gt; u &lt;3&gt; h &lt;3&gt; y &lt;3&gt;</formula><p>RNN unrolled  • one to many, e.g. image description (image to sentence),</p><formula xml:id="formula_115">h &lt;t&gt; = σ W in • [h &lt;t-1&gt; ; u &lt;t&gt; ] + b ,<label>(11)</label></formula><formula xml:id="formula_116">y &lt;t&gt; = W out • h &lt;t&gt; . (<label>12</label></formula><p>• many to one, e.g. sentiment analysis (video to word),</p><p>• many to many, e.g. machine translation (sentence to sentence), like Example 5.1,</p><p>• many to many, e.g. object tracking (video to object location per frame).</p><p>one to many many to one many to many many to many We note that the weights W in , W out and bias b in ( <ref type="formula" target="#formula_115">11</ref>) and <ref type="bibr" target="#b11">(12)</ref> do not change over time, but coincide for all temporal layers of the RNN. Sharing the variables allows the RNN to model variable length sequences, whereas if we had specific parameters for each value of the order parameter, we could not generalize to sequence lengths not seen during training. Typically, σ = tanh is chosen in RNNs, and this does also not vary between the layers. To obtain a complete optimization problem (P ), we still need a loss function L , since the RNN represents only the network F. To this end, each output y &lt;t&gt; is evaluated with a loss function L &lt;t&gt; and the final loss is computed by taking the sum over all time instances</p><formula xml:id="formula_117">L (θ) = Tout t=1</formula><p>L &lt;t&gt; (y &lt;t&gt; (θ)).</p><p>Here, as usual, θ contains the weights W in , W out , and bias b.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Variants of RNNs</head><p>We briefly introduce two popular variants of RNNs.</p><p>In many applications the output at time t should be a prediction depending on the whole input sequence, not only the "earlier" inputs u &lt;i&gt; with i ≤ t. E.g., in speech recognition, the correct interpretation of the current sound as a phoneme may depend on the next few phonemes because of co-articulation and potentially may even depend on the next few words because of the linguistic dependencies between nearby words. As a remedy, we can combine a forward-going RNN and a backward-going RNN, which is then called a Bidirectional RNN, <ref type="bibr" target="#b14">[15,</ref><ref type="bibr">Section 10.3]</ref>. This architecture allows to compute an output y &lt;t&gt; that depends on both the past and the future inputs, but is most sensitive to the input values around time t.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Long term dependencies</head><p>In this section we investigate one of the main challenges, that a RNN can encounter, cf. <ref type="bibr" target="#b14">[15,</ref><ref type="bibr">Section 10.7]</ref>. Consider the following illustrative example.</p><p>Example 5.2</p><p>Predict the next word in the sequence:</p><p>1. The cat, which ..., was ...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>The cats, which ..., were ...</p><p>Here, depending on whether we are talking about one cat or multiple cats the verb has to be adjusted. The "..." part in the sentence can be very extensive, so that the dependence becomes long.</p><p>The gradient from the output y &lt;t&gt; with large t has to propagate back through many layers to affect weights in early layers. Here, the vanishing gradient and exploding gradient problems (cf. </p><formula xml:id="formula_118">∂ θi L ← C • ∂ θi L ∂ θi L .</formula><p>Let us remark that this is a heuristic approach. In contrast, the vanishing gradient problem is more difficult to solve.</p><formula xml:id="formula_119">u &lt;1&gt; h &lt;1&gt; y &lt;1&gt; u &lt;2&gt; h &lt;2&gt; y &lt;2&gt; u &lt;3&gt; h &lt;3&gt; y &lt;3&gt; u &lt;4&gt; h &lt;4&gt; y &lt;4&gt; u &lt;5&gt; h &lt;5&gt; y &lt;5&gt; u &lt;6&gt; h &lt;6&gt; y &lt;6&gt; u &lt;7&gt; h &lt;7&gt; y &lt;7&gt;</formula><p>Figure 37. Illustration of vanishing gradient problem for RNNs. The shading of the nodes indicates the sensitivity over time of the network nodes to the input u &lt;1&gt; (the darker the shade, the greater the sensitivity). The sensitivity decays over time.</p><p>A common remedy is to modify the RNN cell so that it can capture long term dependencies better, and avoids vanishing gradients. Two popular options are Gated Recurrent Unit (GRU) from 2014 <ref type="bibr" target="#b7">[8]</ref>, and the cell architecture as suggested already in 1997 in Long Short Term Memory (LSTM) networks <ref type="bibr" target="#b19">[20]</ref>. The core idea in both cell architectures is to add gating mechanisms. These gates have a significant influence on whether, and how severely, the input and previous hidden state influence the output and new hidden state. Additionally, the gating mechanism helps to solve the vanishing gradient problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1.">Gated Recurrent Unit</head><p>The gated recurrent unit has a reset (or relevance) gate Γ r and an update gate Γ u . The computations for one unit are as follows </p><formula xml:id="formula_120">y &lt;t&gt; = W out • h &lt;t&gt; . output</formula><p>The computations of the hidden state candidate h &lt;t&gt; and the output y &lt;t&gt; resemble the computations in the RNN cell <ref type="bibr" target="#b10">(11)</ref> and <ref type="bibr" target="#b11">(12)</ref>, respectively. However, e.g. if Γ u = 0, then the new hidden state will coincide with the previous hidden state and the candidate will not be taken into account. Also, the GRU has significantly more variables per cell, in comparison with the standard RNN cell.</p><formula xml:id="formula_121">u &lt;t&gt; • σ σ tanh • 1- • h &lt;t-1&gt; h &lt;t&gt; y &lt;t&gt; Γ r Γ u h &lt;t&gt; GRU Figure 38</formula><p>. Architecture of a gated recurrent unit. Weights are omitted in this illustration. A white circle illustrates concatenation, while a circle with a dot represents the Hadamard product and a circle with a plus indicates an addition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2.">Long Short Term Memory</head><p>The key to LSTM networks is that in addition to the hidden state, there also exists a cell state c &lt;t&gt; , which is propagated through the network. It can be understood like a conveyor belt, which only has minor interactions and runs down the entire chain of LSTM cells, see Figure <ref type="figure" target="#fig_33">39</ref>. This allows information to flow through the network easily. In contrast to GRU, the LSTM cell contains three gates: the forget gate Γ f , input gate Γ i and output gate Γ o .</p><formula xml:id="formula_122">Γ f = σ W f • [h &lt;t-1&gt; ; u &lt;t&gt; ] + b f ,</formula><p>forget gate Remark 5.3 Without gates, i.e. Γ f = Γ i = Γ o = 1, the LSTM network has a certain similarity with the ResNet structure, which was developed later than the LSTM, in 2016 in <ref type="bibr" target="#b16">[17]</ref>. This is not so surprising, since both networks aim at solving the vanishing gradient problem. In fact, propagating the cell state has similar effects on the gradients as introducing skip connections.</p><formula xml:id="formula_123">Γ i = σ W i • [</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Language processing</head><p>An important application of RNNs is language processing, e.g. machine translation, see Example 5.1. In such tasks the words need to be represented, so that the RNN can work with them. Furthermore, we need a way to deal with punctuation marks, and an indicator for the end of a sentence.</p><p>To represent the words, we form a dictionary. For the english language we will end up with a vector containing more than 10000 words. Intuitively, we sort the words alphabetically and to simplify computations we use a one-hot representation. E.g., if "the" is the 8367th word in the english dictionary vector, we represent the first input u &lt;1&gt; = 0 . . . 0 1 0 . . . 0 = e 8367 , with the 8367th unit vector. This allows for an easy way to measure correctness in supervised learning and later on we can use the dictionary to recover the words. Additionally, it is common to create a token for unknown words, which are not in the dictionary. Punctuation marks can either be ignored, or we also create tokens for them. However, the dictionary should at least contain an "end of sentence" token to separate sentences from each other.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1.Brain Neuron Structure: electrical inputs are received through dendrites and transmitted via the axon to other cells. There are approximately 86 billion neurons in the human brain. Image modified from: https://www.smartsheet.com/ neural-network-applications.</figDesc><graphic coords="3,89.29,226.00,225.01,113.51" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>σFigure 3 .</head><label>3</label><figDesc>Figure 3. Popular activation functions. Leaky ReLU is displayed for α = 0.1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>L</head><label></label><figDesc>(y, u, W, b) s.t. y = F(u, W, b). (P )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Visualization of k-means algorithm for k = 2 clusters. Data points u (i) are indicated as dots, while cluster centroids µ j are shown as crosses. Top, from left to right: Dataset, random initial cluster centroids µ 1 (red) and µ 2 (blue), every data point is assigned to either the red or blue cluster. Bottom, from left to right: cluster centroids are redefined, every data point is reassigned, cluster centroids are redefined again. Image source: [26, Chapter 10].</figDesc><graphic coords="6,120.54,316.21,354.19,218.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Simple example of a non-convex loss function with a local and a global minimum.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Plots of polynomials of various degrees r (red graph) fitted to the noisy data points (green dots) based on the ground truth (green graph). The model should extend well to the test set data (blue dots). We observe underfitting in the top row for r = 0 (left) and r = 1 (right). In the bottom left with r = 3 reasonable results are achieved, while r = 9 in the bottom right leads to overfitting. Image modified from: [12, Fig. 1].</figDesc><graphic coords="10,89.21,463.34,408.35,151.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Bias-variance trade-off. Image source: [26, Fig. 8.8].</figDesc><graphic coords="11,89.29,408.17,291.67,123.11" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 .</head><label>8</label><figDesc>A simple perceptron (shallow Neural Network) with a two dimensional input.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 11 .</head><label>11</label><figDesc>Figure 11. A feedforward network with 3 hidden layers, layer widths n 0 = 3, n 1 = n 2 = n 3 = 5, n 4 = 1 and depth L = 4. Collecting all variables of this network in a vector will give θ ∈ R 86 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 13 .</head><label>13</label><figDesc>Figure 13. Binary Cross Entropy Loss for label S(u) = 1 (left) and label S(u) = 0 (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 14 .</head><label>14</label><figDesc>Figure 14. Example of the MNIST database. Sample belonging to the digit 7 (left) and 100 samples from all 10 classes (right). Image Source: [4, Fig. 1].</figDesc><graphic coords="20,172.63,528.11,249.99,120.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 15 .</head><label>15</label><figDesc>Figure 15. A feedforward network with 3 hidden layers, i.e. depth L = 4. For the MNIST data set we have n 0 = 784 and n 4 = 10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 17 .</head><label>17</label><figDesc>Figure 17. Architecture of LeNet-5.</figDesc><graphic coords="26,212.22,408.31,170.85,249.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Example 3 . 6</head><label>36</label><figDesc>For this example we have the data matrix</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>5 - 4</head><label>54</label><figDesc>Especially, Y * K = Y K.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 19 .</head><label>19</label><figDesc>Figure 19. An image of size 4×5 is divided in blocks of size 3×3 by moving one pixel at a time either horizontally or vertically, as shown exemplary in red and green. Here, the black square is denoted by the index (1, 1), the red one by (1, 2) and the green one by (2, 1).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 20 .</head><label>20</label><figDesc>Figure 20. A visualization of the convolution of a 7 × 7 images with a 3 × 3 kernel and stride s = 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 23 .</head><label>23</label><figDesc>Figure 23. Original image (left) and blurred image produced by average pooling (right) with a 5 × 5 kernel, stride s = 1 and zero padding with p = 2. Image Source: Laurin Ernst.</figDesc><graphic coords="33,299.30,492.86,204.18,153.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Figure 24 .</head><label>24</label><figDesc>Figure 24. Architecture of AlexNet. ReLU activation is employed in the hidden layers. Image Source: https://datahacker.rs/deep-learning-alexnet-architecture/.</figDesc><graphic coords="34,110.13,308.64,375.00,171.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Figure 25 .</head><label>25</label><figDesc>Figure 25. Illustration of local response normalization. Inter-channel version a) as introduced in [24, Section 3.3] and intra-channel version b). Both for n = 2. For clarification: The red pixel in the top left cube is Y 1,1,1 , while the red pixel in the top row, second from the left cube is Y 1,1,2 and the red pixel in the bottom row, second from the left cube is Y 1,2,1 . Image source: https://towardsdatascience.com/difference-between-localresponse-normalization-and-batch-normalization-272308c034ac.</figDesc><graphic coords="35,151.80,101.03,291.68,174.69" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>Figure 26 .</head><label>26</label><figDesc>Figure 26. Illustration of a simplified ResNet layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head>Figure 28 .</head><label>28</label><figDesc>Figure 28. Illustration of a residual block as introduced in [17].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head>Figure 29 .</head><label>29</label><figDesc>Figure 29. Illustration of a full pre-activation residual block as proposed in [19, Fig.1(b)].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_26"><head></head><label></label><figDesc>we see a comparison of a 1001-layer ResNet built with original residual blocks and a 1001-layer ResNet built with full pre-activation (proposed) residual blocks. This result clearly demonstrates the advantage of full pre-activation for very deep networks, since both training loss and test error can be improved with the proposed residual block.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_27"><head>Figure 30 .</head><label>30</label><figDesc>Figure 30. Training loss (dashed line, left y-axis) and test error (solid line, right y-axis) plotted against the iteration counter for a 1001-layer ResNet on the CIFAR-10 dataset. Here, the original residual block (blue) is compared to the proposed full pre-activation residual block (green). Image Source: [19, Fig.1]</figDesc><graphic coords="39,151.80,274.16,291.68,217.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_28"><head>Figure 31 .</head><label>31</label><figDesc>Figure 31. Illustration of "ResNet18" architecture, built of residual blocks. Image Source: [13].</figDesc><graphic coords="40,89.29,101.03,416.67,85.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_29"><head>Figure 35 .</head><label>35</label><figDesc>Figure 35. Illustration of different types of RNN architectures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_30"><head></head><label></label><figDesc>Figure 36 (left) illustrates the typical bidirectional RNN, with h &lt;t&gt; and g &lt;t&gt; representing the states of the sub-RNNs that move forward and backward through time, respectively. Another variant of RNNs is the Deep RNN, [15, Section 10.5]. As seen in FNNs, Section 2, multiple hidden layers allow the network to have a higher expressiveness. Similarly, a RNN can be made deep by stacking RNN cells, see Figure 36 (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_31"><head>Figure 36 .</head><label>36</label><figDesc>Figure 36. Examples of Bidirectional RNNs and Deep RNNs. Here, the inputs are denoted by x instead of u. Image source: https://stanford.edu/~shervine/teaching/cs-230/.</figDesc><graphic coords="43,89.29,401.35,416.69,233.27" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_32"><head>Γ</head><label></label><figDesc>r = σ W r • [h &lt;t-1&gt; ; u &lt;t&gt; ] + b r , reset gate Γ u = σ W u • [h &lt;t-1&gt; ; u &lt;t&gt; ] + b u ,update gateh &lt;t&gt; = tanh W in • [Γ r h &lt;t-1&gt; ; u &lt;t&gt; ] + b , hidden state candidate h &lt;t&gt; = Γ u h &lt;t&gt; + (1 -Γ u ) h &lt;t-1&gt; , hidden state</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_33"><head>Figure 39 .</head><label>39</label><figDesc>Figure 39. Architecture of a LSTM cell. Weights are omitted in this illustration. A white circle illustrates concatenation, while a circle with a dot represents the Hadamard product and a circle with a plus indicates an addition.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="18,89.29,336.53,416.69,161.71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Neural Networks (RNNs), cf. e.g.<ref type="bibr" target="#b14">[15,</ref> Section 10]  and<ref type="bibr" target="#b11">[12,</ref> Section 8.1], are connectionist models that capture the dynamics of sequences via cycles in the network of nodes. Unlike standard FNNs, recurrent neural networks retain a state that can represent information from an arbitrarily long context window. Translate a given english input sentence u, consisting of T in words u &lt;t&gt; , t = 1, . . . , T in , e.g.</figDesc><table><row><cell cols="2">Example 5.1 (machine translation)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>The</cell><cell>sun</cell><cell>is</cell><cell>shining</cell><cell>today</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Figure 33. Feedforward Neural Network compared to Recurrent Neural Network with input u, output y and hidden computation nodes h. The index is understood as time instance.In RNNs the computation nodes h are often called RNN cells, cf. [15, Section 10.2]. A RNN cell for a time instance t takes as an input u &lt;t&gt; and h &lt;t-1&gt; , and computes the outputs h &lt;t&gt; and y &lt;t&gt; , cf. Figure34. More specifically for all t = 1, . . . , T out</figDesc><table><row><cell>• • •</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>)The equations<ref type="bibr" target="#b10">(11)</ref> and<ref type="bibr" target="#b11">(12)</ref> describe the forward propagation in RNNs. Here, [h &lt;t-1&gt; ; u &lt;t&gt; ] denotes the concatenation of the vectors, and h &lt;0&gt; is set to a vector of zeros, so that we do not need to formulate a special case for t = 1. Depending on the application, a softmax function may be applied to W out h &lt;t&gt; to get the output y &lt;t&gt; .It may happen that input and output have different lengths T in = T out , see e.g. Example 5.1. Depending on the task and the structure of the data, there exist various types of RNN architectures, cf. [12, Section 8.1] and Figure35:</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>y &lt;t&gt;</cell></row><row><cell></cell><cell cols="2">RNN Cell</cell><cell>Weights</cell><cell>Figure 34. Architecture of a RNN cell.</cell></row><row><cell>h &lt;t-1&gt;</cell><cell>Weights</cell><cell>tanh</cell><cell>h &lt;t&gt;</cell></row><row><cell></cell><cell>u &lt;t&gt;</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Section 4) may occur and hinder training. The exploding gradient problem can be solved relatively robust by gradient clipping, see e.g. [15, Section 10.11.1]. The idea is quite simple. If a gradient ∂ θi L , with respect to some variable θ i gets too large, we rescale it. I.e. if ∂ θi L ≥ C ∈ R for a hyperparameter C, we set</figDesc><table /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Deep Neural Nets with Fixed Bias Configuration</title>
		<author>
			<persName><forename type="first">H</forename><surname>Antil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Löhner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Togashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Verma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.01308</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>math.OC</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">An Optimal Time Variable Learning Framework for Deep Neural Networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Antil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Díaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Herberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.08528</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>math.OC</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fractional deep neural network via constrained optimization</title>
		<author>
			<persName><forename type="first">H</forename><surname>Antil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Khatri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Löhner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Verma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning: Science and Technology</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">15003</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A Survey of Handwritten Character Recognition with MNIST and EMNIST</title>
		<author>
			<persName><forename type="first">A</forename><surname>Baldominos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Saez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isasi</surname></persName>
		</author>
		<idno type="DOI">10.3390/app9153169</idno>
		<ptr target="https://www.mdpi.com/2076-3417/9/15/3169" />
	</analytic>
	<monogr>
		<title level="j">Applied Sciences</title>
		<idno type="ISSN">2076-3417</idno>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning long-term dependencies with gradient descent is difficult</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Frasconi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on neural networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="157" to="166" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Vandenberghe</surname></persName>
		</author>
		<title level="m">Convex optimization</title>
		<imprint>
			<publisher>Cambridge university press</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rubanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bettencourt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural ordinary differential equations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<ptr target="https://cs231n.github.io/convolutional-networks/.2023" />
		<title level="m">CS231n Convolutional Neural Networks for Visual Recognition</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Approximation by superpositions of a sigmoidal function</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cybenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mathematics of control, signals and systems</title>
		<imprint>
			<date type="published" when="1989">1989</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Neocognitron: A Self-organizing Neural Network Model for a Mechanism of Pattern Recognition Unaffected by Shift in Position</title>
		<author>
			<persName><forename type="first">K</forename><surname>Fukushima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biological Cybernetics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<ptr target="https://drive.google.com/file/d/16TaFr" />
		<title level="m">Deep Learning Lecture Notes</title>
		<imprint>
			<date type="published" when="2021">6d3eZXNkShgJJxaf6CN7x view. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Convolutional Neural Networks for On-Board Cloud Screening</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ghassemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Magli</surname></persName>
		</author>
		<idno type="DOI">10.3390/rs11121417</idno>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the thirteenth international conference on artificial intelligence and statistics</title>
		<meeting>the thirteenth international conference on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note>JMLR Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Stable architectures for deep neural networks</title>
		<author>
			<persName><forename type="first">E</forename><surname>Haber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ruthotto</surname></persName>
		</author>
		<idno type="DOI">10.1088/1361-6420/aa9a90</idno>
	</analytic>
	<monogr>
		<title level="j">Inverse problems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing humanlevel performance on imagenet classification</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The hadamard product</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Horn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Symp. Appl. Math</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="87" to="169" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<idno>arXiv: 1412.6980</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">ImageNet Classification with Deep Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<ptr target="https://cs229.stanford.edu/notes2022fall/main_notes.pdf.2022" />
		<title level="m">CS229 Lecture Notes</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The perceptron: a probabilistic model for information storage and organization in the brain</title>
		<author>
			<persName><forename type="first">F</forename><surname>Rosenblatt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological review</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page">386</biblScope>
			<date type="published" when="1958">1958</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Transfer learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Torrey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shavlik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of research on machine learning applications and trends: algorithms, methods, and techniques. IGI global</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="242" to="264" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
